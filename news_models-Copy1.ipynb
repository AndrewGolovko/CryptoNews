{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    import news_analysis\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "from glob import glob\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import hyperopt\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'data/news_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datetime_now_str = datetime.datetime.now().strftime(\"%Y%m%dT%H%M\")\n",
    "news_analysis.load_db(file_news=PATH_TO_DATA+'dbnews'+datetime_now_str+'.pkl',\n",
    "                     file_votes=PATH_TO_DATA+'dbvotes'+datetime_now_str+'.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_paths = glob(PATH_TO_DATA+'dbnews*.pkl')\n",
    "votes_paths = glob(PATH_TO_DATA+'dbvotes*.pkl')\n",
    "\n",
    "db_news_dates = np.array([datetime.datetime.strptime(\n",
    "                                re.findall(pattern='([0-9]{8}T[0-9]{4})', string=path)[0], '%Y%m%dT%H%M') \n",
    "                         for path in news_paths])\n",
    "db_votes_dates = np.array([datetime.datetime.strptime(\n",
    "                                re.findall(pattern='([0-9]{8}T[0-9]{4})', string=path)[0], '%Y%m%dT%H%M') \n",
    "                         for path in votes_paths])\n",
    "\n",
    "last_db_news = PATH_TO_DATA+'dbnews'+db_news_dates.max().strftime(\"%Y%m%dT%H%M\")+'.pkl'\n",
    "last_db_votes = PATH_TO_DATA+'dbvotes'+db_votes_dates.max().strftime(\"%Y%m%dT%H%M\")+'.pkl'\n",
    "\n",
    "df_news, df_votes = news_analysis.get_news_and_votes(file_news=last_db_news,file_votes=last_db_votes, currencies=[])\n",
    "\n",
    "datetime_now_str = datetime.datetime.now().strftime(\"%Y%m%dT%H%M\")\n",
    "news_analysis.save_news_and_votes(df_news, df_votes,\n",
    "                                 file_news=PATH_TO_DATA+'dfnews'+datetime_now_str+'.pkl',\n",
    "                                 file_votes=PATH_TO_DATA+'dfvotes'+datetime_now_str+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_paths = glob(PATH_TO_DATA+'dfnews*.pkl')\n",
    "df_votes_paths = glob(PATH_TO_DATA+'dfvotes*.pkl')\n",
    "\n",
    "df_news_dates = np.array([datetime.datetime.strptime(\n",
    "                                re.findall(pattern='([0-9]{8}T[0-9]{4})', string=path)[0], '%Y%m%dT%H%M') \n",
    "                         for path in df_news_paths])\n",
    "df_votes_dates = np.array([datetime.datetime.strptime(\n",
    "                                re.findall(pattern='([0-9]{8}T[0-9]{4})', string=path)[0], '%Y%m%dT%H%M') \n",
    "                         for path in df_votes_paths])\n",
    "\n",
    "last_df_news = PATH_TO_DATA+'dfnews'+df_news_dates.max().strftime(\"%Y%m%dT%H%M\")+'.pkl'\n",
    "last_df_votes = PATH_TO_DATA+'dfvotes'+df_votes_dates.max().strftime(\"%Y%m%dT%H%M\")+'.pkl'\n",
    "\n",
    "df_news, df_votes = news_analysis.load_news_and_votes(file_news=last_df_news,file_votes=last_df_votes)\n",
    "df_news.sort_index(by='created_at', inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def download_prices(start=1405699200, end=9999999999, period_seconds=5*60, currency='BTC', created_at=None, date_modified=None):\n",
    "    '''\n",
    "    download prices (pair USDT/BTC) from poloniex\n",
    "    returns dataframe\n",
    "    '''\n",
    "    address = 'https://poloniex.com/public?command=returnChartData&currencyPair=USDT_'+currency+'&start='+ \\\n",
    "                str(start)+'&end='+str(end)+'&period='+str(period_seconds)\n",
    "    print(address)\n",
    "    df_prices = pd.read_json(address)\n",
    "    return df_prices\n",
    "\n",
    "dates_created = news_analysis.to_datetime(df_news.created_at.values, '%Y-%m-%dT%H:%M')\n",
    "dates_modified = news_analysis.to_datetime(df_votes.date_modified.values, '%m/%d/%Y %H:%M')\n",
    "\n",
    "start_date = min(min(dates_modified), min(dates_created))\n",
    "end_date = max(max(dates_modified), max(dates_created))\n",
    "\n",
    "df_prices = download_prices(start=int(datetime.datetime.timestamp(start_date)), \n",
    "                            end=int(datetime.datetime.timestamp(end_date))+1*60*60)\n",
    "df_prices.to_csv(PATH_TO_DATA+'prices.tmp', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = pd.read_csv(PATH_TO_DATA+'prices.tmp')\n",
    "df_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_emb = news_analysis.get_doc2vec_embeddings(df_news.title.values, vector_size=10, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_Y(titles_embeddings, prices0, prices1, window_width=10, window_stride=10):\n",
    "    X = []\n",
    "    Y = []\n",
    "    max_price = max(prices0.max(), prices1.max())\n",
    "    for i in range(window_width, titles_embeddings.shape[0], window_stride):\n",
    "        X.append(titles_embeddings[i-window_width:i].flatten())\n",
    "        Y.append(prices1[i] - prices0[i])\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_pred, y_test):\n",
    "    score_up = 0\n",
    "    score_down = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] > 0:\n",
    "            score_up += float(y_test[i])\n",
    "        elif y_pred[i] < 0:\n",
    "            score_down -= float(y_test[i])\n",
    "    return score_up, score_down\n",
    "\n",
    "def score_mean(y_pred, y_test, score_tuple=None):\n",
    "    score_up, score_down = score(y_pred, y_test) if type(score_tuple) == type(None) else score_tuple\n",
    "    score_up /= len(y_pred)\n",
    "    score_down /= len(y_pred)\n",
    "    return score_up, score_down\n",
    "\n",
    "def score_f1(y_pred, y_test, score_tuple=None):\n",
    "    score_up, score_down = score(y_pred, y_test) if type(score_tuple) == type(None) else score_tuple\n",
    "    score_up /= len(y_pred)\n",
    "    score_down /= len(y_pred)\n",
    "    if score_up > 0 and score_down > 0:\n",
    "        f1 = 2 * score_up * score_down / (score_up + score_down)\n",
    "    else:\n",
    "        f1 = min(score_up, score_down)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score(model, X, y, scoring_func, k_folds=10, verbose=0):\n",
    "    print('CV splitting') if verbose > 0 else None \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.copy().reshape(-1,1)\n",
    "    \n",
    "    data = np.concatenate((X, y), axis=1)\n",
    "    np.random.shuffle(data)\n",
    "    X, y = data[:,:X.shape[1]], data[:,X.shape[1]:]\n",
    "    \n",
    "    test_sizes = np.ones(k_folds) * (X.shape[0] // k_folds) + \\\n",
    "                 np.concatenate([np.ones(X.shape[0] % k_folds), np.zeros(k_folds - X.shape[0] % k_folds)])\n",
    "    \n",
    "    print('Starting CV') if verbose > 0 else None \n",
    "    scores = []\n",
    "    for i in range(k_folds):\n",
    "        test_indexes = np.arange(test_sizes[:i].sum(), test_sizes[:i+1].sum()).astype(int)\n",
    "        train_indexes = np.concatenate([np.arange(test_indexes[0]), np.arange(test_indexes[-1]+1,X.shape[0])]).astype(int)\n",
    "        \n",
    "        fitted_model = deepcopy(model)\n",
    "        fitted_model.fit(X[train_indexes], y[train_indexes])\n",
    "        y_pred = fitted_model.predict(X[test_indexes])\n",
    "        if type(scoring_func) == list:\n",
    "            scores.append([score(y_pred, y[test_indexes]) for score in scoring_func])\n",
    "        else:\n",
    "            scores.append(scoring_func(y_pred, y[test_indexes]))\n",
    "        if verbose > 0:\n",
    "            print('Score:',scores[-1])\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 5 * 60\n",
    "lag = 6 * 12 * period\n",
    "dates0 = news_analysis.to_datetime(df_news.created_at.values)\n",
    "dates1 = dates0 + datetime.timedelta(0, lag)\n",
    "prices0 = news_analysis.get_prices_at_date(dates0, df_prices)\n",
    "prices1 = news_analysis.get_prices_at_date(dates1, df_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.any(prices1 == 0):\n",
    "    end_ix = (prices1 == 0).argmax()\n",
    "else:\n",
    "    end_ix = prices1.shape[0]\n",
    "    \n",
    "titles_emb = titles_emb[:end_ix]\n",
    "prices0 = prices0[:end_ix]\n",
    "prices1 = prices1[:end_ix]\n",
    "dates0 = dates0[:end_ix]\n",
    "dates1 = dates1[:end_ix]\n",
    "X, Y = make_X_Y(titles_emb, prices0, prices1, window_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate(diff):\n",
    "    x0 = 0\n",
    "    x = [x0]\n",
    "    for d in diff:\n",
    "        x.append(x[-1] + d)\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "\n",
    "    def __init__(self, X, y, X_test, y_test):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    # Calculate cross validation score (default is10-fold CV)\n",
    "    def __call__(self, X, cv=10):\n",
    "        # Make the SVM classifire with  the specific 'C' and 'degree'\n",
    "        xgb = XGBRegressor(max_depth=int(X['max_depth']),\n",
    "                           #learning_rate=X['learning_rate'],\n",
    "                           n_estimators=int(10**X['n_estimators']),\n",
    "                           n_jobs=-1)\n",
    "        \n",
    "        xgb.fit(self.X, self.y)\n",
    "        y_pred = xgb.predict(self.X_test)\n",
    "        s = score(y_pred, self.y_test)\n",
    "        s_mean = score_mean(y_pred, self.y_test, s)\n",
    "        f1 = score_f1(y_pred, self.y_test, s)\n",
    "        print(X)\n",
    "        print('Sum score:', sum(s))\n",
    "        print('Sum score up and down:', s)\n",
    "        print('Mean score up and down:', score_mean)\n",
    "        print('F1:', f1)\n",
    "        return - f1    # for minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of design variables\n",
    "parameter_space = {#'learning_rate':hp.loguniform(\"learning_rate\", 0.01, 0.5),\n",
    "                   'max_depth': hp.quniform('max_depth', 1, 5, q=1),\n",
    "                   'n_estimators': hp.uniform('n_estimators', 1, 3)}\n",
    "\n",
    "# Objective function\n",
    "f = ObjectiveFunction(X_train, Y_train, X_test, Y_test)\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(f, parameter_space, \n",
    "                     algo=hyperopt.tpe.suggest, max_evals=10, trials=trials, verbose=1)\n",
    "print(\"best estimate parameters\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb = XGBRegressor(max_depth=int(2.0),\n",
    "                           #learning_rate=X['learning_rate'],\n",
    "                           n_estimators=int(10**1.690026595292681),\n",
    "                           n_jobs=-1)\n",
    "scores = cv_score(xgb, X_train, Y_train, [score, score_mean, score_f1], k_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, Y_train)\n",
    "Y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(dates1[-Y_pred.shape[0]:], prices0[-Y_pred.shape[0]:] + Y_pred)\n",
    "\n",
    "# plt.plot(dates1, prices1)\n",
    "plt.plot(dates1[-Y_pred.shape[0]:], prices1[-Y_pred.shape[0]:])\n",
    "plt.legend(['prediction', 'true'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = []\n",
    "thresholds = np.arange(30)\n",
    "for threshold in thresholds:\n",
    "    gain.append(integrate(Y_test[np.abs(Y_pred) > threshold] * np.sign(Y_pred[np.abs(Y_pred) > threshold]) \\\n",
    "                            - 0.001 * (prices0[-Y_test.shape[0]:][np.abs(Y_pred) > threshold]))[-1])\n",
    "\n",
    "gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "threshold = 10\n",
    "# plt.plot(integrate(Y_test[np.abs(Y_pred) > threshold] * np.sign(Y_pred[np.abs(Y_pred) > threshold]) \\\n",
    "#                    - 0.001 * (prices0[-Y_test.shape[0]:][np.abs(Y_pred) > threshold] + prices1[-Y_test.shape[0]:][np.abs(Y_pred) > threshold])))\n",
    "plt.plot(integrate(Y_test[np.abs(Y_pred) > threshold] * np.sign(Y_pred[np.abs(Y_pred) > threshold]) \\\n",
    "                   - 0.001 * (prices0[-Y_test.shape[0]:][np.abs(Y_pred) > threshold])))\n",
    "plt.legend(['prediction', 'true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(dates1[-Y_pred.shape[0]:], integrate(Y_pred))\n",
    "plt.plot(dates1[-Y_test.shape[0]:], integrate(Y_test))\n",
    "plt.legend(['prediction', 'true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((Y - Y.mean()) / Y.std(), bins=np.linspace(-2,2, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df_news, df_prices, \n",
    "                 lag=6*60*60, \n",
    "                 vector_size=10, window=2, min_count=1, \n",
    "                 window_width=10, window_stride=1, \n",
    "                 workers=4, verbose=0):\n",
    "    \n",
    "    print('Extracting dates') if verbose>0 else None\n",
    "    dates0 = news_analysis.to_datetime(df_news.created_at.values)\n",
    "    dates1 = dates0 + datetime.timedelta(0, lag)\n",
    "    print('Extracting prices') if verbose>0 else None\n",
    "    prices0 = news_analysis.get_prices_at_date(dates0, df_prices)\n",
    "    prices1 = news_analysis.get_prices_at_date(dates1, df_prices)\n",
    "    \n",
    "    if np.any(prices1 == 0):\n",
    "        end_ix = (prices1 == 0).argmax()\n",
    "    else:\n",
    "        end_ix = prices1.shape[0]\n",
    "        \n",
    "    print('Creating titles embeddings') if verbose>0 else None\n",
    "    titles_emb = news_analysis.get_doc2vec_embeddings(df_news.title.values, \n",
    "                                                      vector_size=vector_size, \n",
    "                                                      window=window, \n",
    "                                                      min_count=min_count, \n",
    "                                                      workers=workers)\n",
    "    print('Creating dataset') if verbose>0 else None\n",
    "    titles_emb = titles_emb[:end_ix]\n",
    "    prices0 = prices0[:end_ix]\n",
    "    prices1 = prices1[:end_ix]\n",
    "    dates0 = dates0[:end_ix]\n",
    "    dates1 = dates1[:end_ix]\n",
    "    X, Y = make_X_Y(titles_emb, prices0, prices1, window_width=window_width, window_stride=window_stride)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, shuffle=False)\n",
    "    \n",
    "    return dates0, dates1, prices0, prices1, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction_XGB:\n",
    "\n",
    "    def __init__(self, df_news, df_prices):\n",
    "        self.df_news = df_news\n",
    "        self.df_prices = df_prices\n",
    "\n",
    "    # Calculate cross validation score (default is10-fold CV)\n",
    "    def __call__(self, X, cv=10):\n",
    "        dates0, dates1, prices0, prices1, X_train, X_test, Y_train, Y_test = make_dataset(\n",
    "                                                        self.df_news,\n",
    "                                                        self.df_prices,\n",
    "                                                        lag=int(X['lag'])*12*5*60,\n",
    "                                                        vector_size=10,#int(X['vector_size']),\n",
    "                                                        window=2,#int(X['window']),\n",
    "                                                        min_count=int(X['min_count']),\n",
    "                                                        window_width=7,#int(X['window_width']),\n",
    "                                                        window_stride=1,#int(X['window_stride']),\n",
    "                                                        workers=4,\n",
    "                                                        verbose=0)\n",
    "        \n",
    "        xgb = XGBRegressor(max_depth=int(X['max_depth']),\n",
    "                           #learning_rate=X['learning_rate'],\n",
    "                           n_estimators=int(10**X['n_estimators']),\n",
    "                           n_jobs=4)\n",
    "        \n",
    "        scores = cv_score(xgb, X_train, Y_train, (lambda x, y: score_mean(x, y)[1]), k_folds=10, verbose=1)\n",
    "        print(X)\n",
    "        mean_score = np.mean(scores)\n",
    "        print('Mean score:', mean_score)\n",
    "        print()\n",
    "        return - mean_score    # for minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Definition of design variables\n",
    "parameter_space = {#'learning_rate':hp.loguniform(\"learning_rate\", 0.01, 0.5),\n",
    "                    'max_depth': hp.quniform('max_depth', 2, 5, q=1),\n",
    "                    'n_estimators': hp.uniform('n_estimators', 1, 1.2), \n",
    "                    'lag': hp.quniform('lag', 4, 12, q=2),\n",
    "#                     'vector_size':hp.quniform('vector_size', 6, 10, q=1),\n",
    "#                     'window':hp.quniform('window', 2,5,q=1),\n",
    "                    'min_count':hp.quniform('min_count', 1,3,q=1)}\n",
    "#                     'window_width':hp.quniform('window_width', 6,10,q=1),\n",
    "#                     'window_stride':hp.quniform('window_stride', 1,5,q=1)}\n",
    "\n",
    "# Objective function\n",
    "f = ObjectiveFunction_XGB(df_news, df_prices)\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(f, parameter_space, \n",
    "                     algo=hyperopt.tpe.suggest, max_evals=25, trials=trials, verbose=1)\n",
    "print(\"best estimate parameters\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates0, dates1, prices0, prices1, X_train, X_test, Y_train, Y_test = make_dataset(df_news, df_prices,\n",
    "                                                                                  lag=8*12*5*60,\n",
    "                                                                                   vector_size=10,\n",
    "                                                                                  window=2,\n",
    "                                                                                  min_count=3,\n",
    "                                                                                  window_width=7,\n",
    "                                                                                  window_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb = XGBRegressor(n_jobs=-1, max_depth=2, n_estimators=int(10**1.1982429076195422))\n",
    "xgb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_f1(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = []\n",
    "thresholds = np.arange(10)\n",
    "for threshold in thresholds:\n",
    "    gain.append(integrate(Y_test[-Y_pred > threshold] * np.sign(Y_pred[-Y_pred > threshold]) \\\n",
    "                   - 0.001 * (prices0[-Y_test.shape[0]:][-Y_pred > threshold]))[-1])\n",
    "np.array(gain).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "threshold = 4\n",
    "# plt.plot(integrate(Y_test[np.abs(Y_pred) > threshold] * np.sign(Y_pred[np.abs(Y_pred) > threshold]) \\\n",
    "#                    - 0.001 * (prices0[-Y_test.shape[0]:][np.abs(Y_pred) > threshold] + prices1[-Y_test.shape[0]:][np.abs(Y_pred) > threshold])))\n",
    "# plt.plot(integrate(Y_test[np.abs(Y_pred) > threshold] * np.sign(Y_pred[np.abs(Y_pred) > threshold]) \\\n",
    "#                    - 0.001 * (prices0[-Y_test.shape[0]:][np.abs(Y_pred) > threshold])))\n",
    "integral = integrate(Y_test[-Y_pred > threshold] * np.sign(Y_pred[-Y_pred > threshold]) \\\n",
    "                   - 0.001 * (prices0[-Y_test.shape[0]:][-Y_pred > threshold]))\n",
    "plt.plot(integral)\n",
    "plt.grid()\n",
    "print(integral[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_TO_DATA+'logs/XGB_20181028T1603.csv')\n",
    "df.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1:].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = smf.ols('score ~ lag + max_depth + min_count + n_estimators + vector_size + window + window_stride + window_width', \n",
    "             data=df)\n",
    "fitted = m1.fit()\n",
    "fitted.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
